{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5885e644",
   "metadata": {},
   "source": [
    "# Part 1: Research and Data Sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d533a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of reliable data sources about construction and infrastructure projects in California:\n",
      "https://www.caleprocure.ca.gov/pages/index.aspx\n",
      "https://www.constructionbidsource.com/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_california_construction_sources():\n",
    "    # URLs of relevant websites\n",
    "    urls = [\n",
    "        \"https://dot.ca.gov/programs/construction\",\n",
    "        \"https://www.caleprocure.ca.gov/pages/index.aspx\",\n",
    "        \"https://www.constructionbidsource.com/\",\n",
    "        \"https://www.constructionwire.com/\",\n",
    "        # Add more URLs as needed\n",
    "    ]\n",
    "    \n",
    "    sources = []\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Extract relevant information from the website\n",
    "            if \"dot.ca.gov\" in url:\n",
    "                # Example: California Department of Transportation (Caltrans)\n",
    "                project_links = soup.find_all('a', class_='more-link')\n",
    "                for link in project_links:\n",
    "                    sources.append(link['href'])\n",
    "            elif \"caleprocure.ca.gov\" in url:\n",
    "                # Example: California State Contracts Register\n",
    "                sources.append(url)\n",
    "            elif \"constructionbidsource.com\" in url or \"constructionwire.com\" in url:\n",
    "                # Example: Construction Bid Source or ConstructionWire\n",
    "                sources.append(url)\n",
    "            # Add more conditions as needed for other websites\n",
    "            \n",
    "    return sources\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    construction_sources = scrape_california_construction_sources()\n",
    "    print(\"List of reliable data sources about construction and infrastructure projects in California:\")\n",
    "    for source in construction_sources:\n",
    "        print(source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0cecb",
   "metadata": {},
   "source": [
    "# Part 2: Data Extraction and Standardization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348a2161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Major Projects | Richmond, CA - Official Website', 'description': 'Information about projects under review by the Planning Division'}\n",
      "{'title': 'Projects | Mill Valley, CA', 'description': ''}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_data_from_source(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract relevant information from the website\n",
    "        # Replace this with specific extraction logic based on the website structure\n",
    "        data = {\n",
    "            'title': soup.find('title').text.strip(),\n",
    "            'description': soup.find('meta', attrs={'name': 'description'})['content'].strip(),\n",
    "            # Add more fields as needed\n",
    "        }\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "        return None\n",
    "\n",
    "# List of URLs to scrape data from\n",
    "urls = [\n",
    "    'https://www.ci.richmond.ca.us/1404/Major-Projects',\n",
    "    'https://www.cityofmillvalley.org/258/Projects'\n",
    ",\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Scraping data from each source\n",
    "extracted_data = []\n",
    "for url in urls:\n",
    "    data = scrape_data_from_source(url)\n",
    "    if data:\n",
    "        extracted_data.append(data)\n",
    "\n",
    "# Printing extracted data (for demonstration)\n",
    "for data in extracted_data:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2504c478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_id': 'ca34beb4-9572-45c6-8b09-872235fd02be', 'aug_id': '24bfc7b4-2813-4d9c-848e-929eea45a45b', 'country_name': 'California', 'country_code': 'USA', 'map_coordinates': {'type': 'Point', 'coordinates': [0, 0]}, 'url': '', 'region_name': 'California', 'region_code': 'CA', 'title': 'Major Projects | Richmond, CA - Official Website', 'description': 'Information about projects under review by the Planning Division'}\n",
      "{'original_id': '4119840c-34a2-4ab3-a3c0-fac8ec345ffd', 'aug_id': '9fba74c3-be89-4643-a3e7-c4582e8a8778', 'country_name': 'California', 'country_code': 'USA', 'map_coordinates': {'type': 'Point', 'coordinates': [0, 0]}, 'url': '', 'region_name': 'California', 'region_code': 'CA', 'title': 'Projects | Mill Valley, CA', 'description': ''}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def standardize_data(data):\n",
    "    standardized_data = {\n",
    "        'original_id': str(uuid.uuid4()),  # Generate a unique ID for each source\n",
    "        'aug_id': str(uuid.uuid4()),       # Generate a UUID for augmentation ID\n",
    "        'country_name': 'California',      # Assuming data is from California\n",
    "        'country_code': 'USA',             # Assuming country code for USA\n",
    "        'map_coordinates': {'type': 'Point', 'coordinates': [0, 0]},  # Placeholder coordinates\n",
    "        'url': data.get('url', ''),        # Assuming 'url' is provided in data\n",
    "        'region_name': 'California',       # Assuming region name is California\n",
    "        'region_code': 'CA',               # Assuming region code for California\n",
    "        'title': data.get('title', ''),    # Standardize title\n",
    "        'description': data.get('description', ''),  # Standardize description\n",
    "        # Add more fields and standardize them as per guidelines\n",
    "    }\n",
    "    return standardized_data\n",
    "\n",
    "# Standardizing extracted data\n",
    "standardized_data = [standardize_data(data) for data in extracted_data]\n",
    "\n",
    "# Printing standardized data (for demonstration)\n",
    "for data in standardized_data:\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef805037",
   "metadata": {},
   "source": [
    "# Part 3: Automation and Continuous Updating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4995d6d",
   "metadata": {},
   "source": [
    "Task: Propose a system for automating the data scraping and standardization processes.\n",
    "Details:\n",
    "\n",
    "Explain how the data sources will be continuously updated.\n",
    "\n",
    "Describe the use of cron jobs or similar scheduling tools for ongoing data updates.\n",
    "\n",
    "Ensure your methodology adheres to a production environment's standards.\n",
    "\n",
    "Evaluation Criteria\n",
    "\n",
    "● Scalability: Ability to scrape multiple sources effectively.\n",
    "\n",
    "● Adherence to Standards: Conformity with the provided data standards; penalties for\n",
    "deviation.\n",
    "\n",
    "● Automation and Continuity: Quality of the proposal for continuous data updating,\n",
    "including details on cron monitoring and production environment suitability.\n",
    "Deliverables\n",
    "\n",
    "Candidates should share a Google Drive folder containing:\n",
    "\n",
    "1. Python Scripts: The actual code used for data scraping and standardization.\n",
    "\n",
    "2. Documentation: Detailed explanations of the scripts and methodologies.\n",
    "\n",
    "3. Sample Datasets: Examples of the data extracted and standardized.\n",
    "\n",
    "4. Production Environment Plan: A document detailing the implementation of cron\n",
    "\n",
    "monitoring and how the system will operate in a production environment.\n",
    "\n",
    "Notes to Candidates\n",
    "\n",
    "● Pay close attention to the data standards and ensure your methods are scalable and\n",
    "suitable for a production environment.\n",
    "\n",
    "● Clearly articulate your use of AI or machine learning models, specifically in the context of\n",
    "data sourcing and any preprocessing tasks.\n",
    "\n",
    "● Demonstrate a thoughtful approach to continuous data updating and monitoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
